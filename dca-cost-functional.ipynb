{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2afd40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "# positive class: 1 \n",
    "# negative class: 0\n",
    "\n",
    "# number of examples in each class\n",
    "\n",
    "nN = 4*2000\n",
    "nP = 2000\n",
    "n = nN + nP\n",
    "\n",
    "# scores for each class generated from two normal distributions\n",
    "scoresN=np.random.normal(loc=0.4, scale=0.12, size=nN)\n",
    "scoresP=np.random.normal(loc=0.6, scale=0.12, size=nP)\n",
    "\n",
    "# plot the scores - one histogram for each class\n",
    "def plotScoreDistributions(scores, labels):\n",
    "    plt.hist(scores[labels==0], density=True, bins=30, alpha=0.5, label=\"Negative class\", color=\"skyblue\")\n",
    "    plt.hist(scores[labels==1], density=True, bins=30, alpha=0.5, label=\"Positive class\", color=\"green\")\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "\n",
    "# create labels for the two classes\n",
    "labelsN=np.zeros(nN)\n",
    "labelsP=np.ones(nP)\n",
    "\n",
    "# create data frame of scores and labels\n",
    "scores= np.concatenate((scoresN, scoresP))\n",
    "labels= np.concatenate((labelsN, labelsP))\n",
    "\n",
    "d = {'score': scores, 'label':labels}\n",
    "\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "df = df.set_index(\"score\")\n",
    "df = df.sort_values('score')\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# checking\n",
    "df.size\n",
    "#print(df.head(n=10))\n",
    "#print(df.tail(n=10))\n",
    "print(min(df.score))\n",
    "print(max(df.score))\n",
    "\n",
    "print('class distribution:', nP/(nN+nP))\n",
    "\n",
    "plotScoreDistributions(df.score, df.label)\n",
    "\n",
    "# fix scores outside 0 and 1\n",
    "print('Min:', min(df.score), \" Max: \", max(df.score))\n",
    "\n",
    "df.loc[df['score'] < 0, 'score'] = 0\n",
    "df.loc[df['score'] > 1, 'score'] = 1\n",
    "print('Min:', min(df.score), \" Max: \", max(df.score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6155b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be3d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate AUC\n",
    "\n",
    "from sklearn import metrics\n",
    "#xx = df.dropna(subset=\"label\")\n",
    "#auc = metrics.roc_auc_score(xx['label'], xx['score'])\n",
    "\n",
    "auc = metrics.roc_auc_score(df['label'], df['score'])\n",
    "print(\"AUC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bbd0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate scores\n",
    "from scipy.stats import norm\n",
    "\n",
    "def myCalibrate(score):\n",
    "    piP = nP/(nN+nP)\n",
    "\n",
    "    scoresCal = piP*norm.pdf(score, 0.55, 0.15)/((1-piP)*norm.pdf(score, 0.45, 0.15)+piP*norm.pdf(score, 0.55, 0.15))\n",
    "    plt.scatter(score, scoresCal)\n",
    "    plt.xlabel('Original score')\n",
    "    plt.ylabel('Calibrated score')\n",
    "\n",
    "    return(scoresCal)\n",
    "\n",
    "df['scoreCal'] = myCalibrate(df.score)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a872e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the calibrated score distributions\n",
    "\n",
    "plotScoreDistributions(df.scoreCal, df.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate TPR and FPR\n",
    "\n",
    "# init fpr and tpr columns\n",
    "df.fpr=np.nan\n",
    "df.tpr=np.nan\n",
    "\n",
    "# loop through each examples (sorted by score) and calculate tpr and fpr for with that score as the threshold \n",
    "prevScore = -1\n",
    "prevLabel = -1\n",
    "for i in range(0,df.shape[0]):\n",
    "\n",
    "    threshold = df.loc[i,'score']\n",
    "    \n",
    "    # find indexes with examples predicted as true for this threshold\n",
    "    i_pred = np.where(df.score >= threshold, 1, 0)\n",
    "    \n",
    "    # calculate fpr and tpr\n",
    "    label = df.label[i]\n",
    "    fp = np.sum((i_pred==1) & (df.label == 0))\n",
    "    tp = np.sum((i_pred==1) & (df.label == 1))\n",
    "    \n",
    "    df.loc[i, 'fpr'] = fp / nN\n",
    "    df.loc[i, 'tpr'] = tp / nP\n",
    "\n",
    "\n",
    "# past the highest score and no examples classified as true \n",
    "numrows = df.shape[0]\n",
    "df.loc[numrows,'fpr'] = 0\n",
    "df.loc[numrows,'tpr'] = 0\n",
    "\n",
    "print(df.head(n=10))\n",
    "print(df.tail(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f7f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# plot roc curve\n",
    "ax = df.plot(kind = 'line', x = 'fpr', y ='tpr', legend=False, color='skyblue')\n",
    "ax.set_ylabel(\"TPR\")\n",
    "ax.set_xlabel(\"FPR\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f45c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### plotting the cost curve\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Calculate loss for different costs\n",
    "piN = nN/(nN+nP)\n",
    "piP = 1 - piN\n",
    "\n",
    "b = 2\n",
    "c_0 = 1\n",
    "c = c_0 / 2\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "    \n",
    "# plot line in cost space for each F0 F1 pair in ROC space\n",
    "for i in range(1,df.shape[0]):\n",
    "\n",
    "    gradient = 2*(piN*df.loc[i,'fpr'] - piP*(1-df.loc[i,'tpr']))\n",
    "    intercept = 2*piP * (1-df.loc[i,'tpr'])\n",
    "    ax.axline((0, intercept), slope=gradient, color='C0')\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be54e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost curve without plotting all the lines\n",
    "\n",
    "# get best loss for each cost\n",
    "loss_costs = []\n",
    "for c in np.arange(0,1.01, 0.01):\n",
    "    \n",
    "    #loss = 2*(c*pi_1*(1-df['tpr']) + (1-c)*pi_0*df['fpr'])\n",
    "    loss = 2*((1-c)*piP*(1-df['tpr']) + c*piN*df['fpr'])\n",
    "    \n",
    "    minLoss = min(loss)\n",
    "\n",
    "    # the point with lowest loss doesn't have c==t\n",
    "    print(c, ': ', df.score[np.argmin(loss)])\n",
    "\n",
    "    loss_cost = {'cost':c, 'loss':minLoss}\n",
    "    loss_costs.append(loss_cost)\n",
    "\n",
    "dfLossCost = pd.DataFrame(loss_costs)\n",
    "\n",
    "ax = dfLossCost.plot(kind = 'line', x = 'cost', y ='loss', legend=False, color='skyblue', ylim=[0,1])\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xlabel(\"Cost\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c561c6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate brier loss for the original score and the calibrated scores\n",
    "\n",
    "df['brier_lossx'] = 2*(1-df['score'])*piP*(1-df['tpr']) + 2*df['score']*piN*df['fpr']\n",
    "df['brier_loss_calx'] = 2*(1-df['scoreCal'])*piP*(1-df['tpr']) + 2*df['scoreCal']*piN*df['fpr']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf8b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cost curve and brier curve (of uncalibrated scores)\n",
    "ax = dfLossCost.plot(kind = 'line', x = 'cost', y ='loss', color='skyblue', ylim=[0,1], label='Model cost curve')\n",
    "df.plot(kind = 'line', x = 'score', y ='brier_loss', ax=ax, color='C7', linestyle='--', label='Model Brier curve')\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xlabel(\"Cost\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801a4b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# same plot but checking that the calibrated scores version is the same as the cost curve\n",
    "\n",
    "ax = dfLossCost.plot(kind = 'line', x = 'cost', y ='loss', legend=False, color='skyblue', ylim=[0,1])\n",
    "df.plot(kind = 'line', x = 'score', y ='brier_loss', ax=ax, color='C7', linestyle='-', label='Model Brier curve')\n",
    "df.plot(kind = 'line', x = 'scoreCal', y ='brier_loss_cal', ax=ax, color='C8', linestyle='--', label='Model Brier curve cal')\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xlabel(\"Cost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e98275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate net benefit for DCA\n",
    "\n",
    "#df['netben'] = df.tpr*nP/n - (df.fpr*nN/n)*(df.score/(1-df.score))\n",
    "df['netben'] = df['tpr']*nP/n - (df['fpr']*nN/n)*(df['score']/(1-df['score']))\n",
    "df['netben_treatnone'] = 0/n - (0/n)*(df['score']/(1-df['score']))\n",
    "df['netben_treatall'] = nP/n - (nN/n)*(df['score']/(1-df['score']))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f00f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot DCA curve\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(-1, 1)\n",
    "df.plot(kind = 'line', x = 'score', y ='netben', ax=ax, color='C2', linestyle='-', label='Model, standard DCA')\n",
    "df.plot(kind = 'line', x = 'score', y ='netben_treatall', ax=ax, color='C2', linestyle='--', label='Treat all standard DCA')\n",
    "ax.axline((0, 0), slope=0, color='C3', markersize=1, linestyle='-', linewidth=1, label=\"Treat none\")\n",
    "ax.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f36013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    df.loc[i,'brier_loss_treatall'] = 2*(1-df.loc[i,'score'])*pi_0*(1-1) + 2*df.loc[i,'score']*pi_1*1\n",
    "#    df.loc[i,'brier_loss_treatnone'] = 2*(1-df.loc[i,'score'])*pi_0*(1-0) + 2*df.loc[i,'score']*pi_1*0\n",
    "\n",
    "    # df.loc[i,'NB_loss'] = 1*pi_0*(1-df.loc[i,'f0']) + ((1-df.loc[i,'score'])/(df.loc[i,'score']))*pi_1*df.loc[i,'f1']\n",
    "    \n",
    "    # df.loc[i,'max_brier_loss'] = 2*df.loc[i,'score']*pi_0*(1-0) + 2*(1-df.loc[i,'score'])*pi_1*1\n",
    "    # df.loc[i,'max_NB_loss'] = 1*pi_0*(1-0) + ((1-df.loc[i,'score'])/(df.loc[i,'score']))*pi_1*1\n",
    "\n",
    "    # df.loc[i,'NB_loss_treatall'] = 1*pi_0*(1-1) + ((1-df.loc[i,'score'])/(df.loc[i,'score']))*pi_1*1\n",
    "    # df.loc[i,'NB_loss_treatnone'] = 1*pi_0*(1-0) + ((1-df.loc[i,'score'])/(df.loc[i,'score']))*pi_1*0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
